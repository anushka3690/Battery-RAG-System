{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9d3fa8",
   "metadata": {},
   "source": [
    "# ðŸ”‹ Battery RAG System - Following Architecture Diagram\n",
    "\n",
    "## ðŸ—ï¸ RAG Architecture Flow:\n",
    "1. **Data Input** â†’ **Document Chunking** â†’ **Embedding Model** â†’ **Vector Store**\n",
    "2. **User Query** â†’ **Semantic Search** â†’ **Retrieved Context**  \n",
    "3. **Query + Context** â†’ **Prompt Template** â†’ **LLM** â†’ **Response**\n",
    "\n",
    "Each step implemented in small, clear cells for easy understanding and API integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90642fce",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd1f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Basic data processing imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "print(\"Basic imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da28967d",
   "metadata": {},
   "source": [
    "## ðŸ§  Step 2: Vector & LLM Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef7cdaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iaman\\anaconda3\\envs\\LangChain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector processing imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Vector database and embeddings\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "print(\"Vector processing imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca96c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain imports loaded\n"
     ]
    }
   ],
   "source": [
    "# LangChain for RAG pipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "print(\"LangChain imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e87fef0",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 3: Data Input & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86e326c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading battery data...\n",
      "Loaded 10000 battery records\n",
      "Columns: ['Cell_ID', 'Cell_Type', 'Nominal_Voltage_V', 'Capacity_Ah', 'Internal_Resistance_mOhm', 'Gravimetric_Energy_Density_Wh/kg', 'Volumetric_Energy_Density_Wh/L', 'Thermal_Runaway_Temp_C', 'Cathode_Material', 'Anode_Material', 'Separator_Material']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iaman\\AppData\\Local\\Temp\\ipykernel_8264\\2939988610.py:4: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df = df.apply(pd.to_numeric, errors='ignore').dropna().reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Load battery dataset\n",
    "print(\"Loading battery data...\")\n",
    "df = pd.read_csv('battery_data_10000_rows.csv')\n",
    "df = df.apply(pd.to_numeric, errors='ignore').dropna().reset_index(drop=True)\n",
    "print(f\"Loaded {len(df)} battery records\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08482355",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 4: Document Chunking & Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a25ccc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document formatting function ready\n"
     ]
    }
   ],
   "source": [
    "# Document formatting function\n",
    "def format_battery_document(row):\n",
    "    \"\"\"Convert battery row into structured text document\"\"\"\n",
    "    return f\"\"\"Battery ID: {row.iloc[0]}\n",
    "Type: {row.iloc[1] if len(row) > 1 else 'Unknown'}\n",
    "Voltage: {row.iloc[2] if len(row) > 2 else 'Unknown'} V\n",
    "Capacity: {row.iloc[3] if len(row) > 3 else 'Unknown'} Ah\n",
    "Energy Density: {row.iloc[4] if len(row) > 4 else 'Unknown'} Wh/kg\"\"\"\n",
    "\n",
    "print(\"Document formatting function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fee0e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating structured documents...\n",
      "Created 10000 structured documents\n",
      "Sample document:\n",
      "Battery ID: BAT-00001\n",
      "Type: Pouch\n",
      "Voltage: 3.85 V\n",
      "Capacity: 5.67 Ah\n",
      "Energy Density: 16.5 Wh/kg\n"
     ]
    }
   ],
   "source": [
    "# Create structured documents from data\n",
    "print(\"Creating structured documents...\")\n",
    "documents = [format_battery_document(row) for _, row in df.iterrows()]\n",
    "print(f\"Created {len(documents)} structured documents\")\n",
    "print(f\"Sample document:\\n{documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68848ee9",
   "metadata": {},
   "source": [
    "## ðŸ§  Step 5: Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "814fc406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Embedding model loaded\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f64ba4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [01:13<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings with shape: (10000, 384)\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for all documents\n",
    "print(\"Creating embeddings...\")\n",
    "embeddings = embedding_model.encode(documents, show_progress_bar=True)\n",
    "print(f\"Created embeddings with shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd545fe",
   "metadata": {},
   "source": [
    "## ðŸ—„ï¸ Step 6: Vector Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8206aa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up vector store...\n",
      "Vector store initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB vector store\n",
    "print(\"Setting up vector store...\")\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Clean up existing collection\n",
    "try:\n",
    "    chroma_client.delete_collection(name=\"battery_rag\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = chroma_client.create_collection(name=\"battery_rag\")\n",
    "print(\"Vector store initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7bcd6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing documents in vector store...\n",
      "   Batch 1: 1000 documents\n",
      "   Batch 2: 1000 documents\n",
      "   Batch 3: 1000 documents\n",
      "   Batch 4: 1000 documents\n",
      "   Batch 5: 1000 documents\n",
      "   Batch 6: 1000 documents\n",
      "   Batch 7: 1000 documents\n",
      "   Batch 8: 1000 documents\n",
      "   Batch 9: 1000 documents\n",
      "   Batch 10: 1000 documents\n",
      "Stored 10000 documents in vector store\n"
     ]
    }
   ],
   "source": [
    "# Store documents and embeddings in batches\n",
    "print(\"Storing documents in vector store...\")\n",
    "batch_size = 1000\n",
    "\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    end_idx = min(i + batch_size, len(documents))\n",
    "    collection.add(\n",
    "        documents=documents[i:end_idx],\n",
    "        embeddings=embeddings[i:end_idx].tolist(),\n",
    "        ids=[str(j) for j in range(i, end_idx)]\n",
    "    )\n",
    "    print(f\"   Batch {i//batch_size + 1}: {end_idx - i} documents\")\n",
    "\n",
    "print(f\"Stored {collection.count()} documents in vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5f9c7",
   "metadata": {},
   "source": [
    "## ðŸ” Step 7: Semantic Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bced46b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Semantic search function ready\n"
     ]
    }
   ],
   "source": [
    "# Semantic search function\n",
    "def semantic_search(query, top_k=3):\n",
    "    \"\"\"Perform semantic search to retrieve relevant documents\"\"\"\n",
    "    print(f\"Searching for: '{query}'\")\n",
    "    \n",
    "    # Create query embedding\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Search vector store\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    retrieved_docs = results['documents'][0]\n",
    "    print(f\"Retrieved {len(retrieved_docs)} relevant documents\")\n",
    "    return retrieved_docs\n",
    "\n",
    "print(\"ðŸ” Semantic search function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfbed71",
   "metadata": {},
   "source": [
    "## ðŸ“ Step 8: Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cded1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template created\n"
     ]
    }
   ],
   "source": [
    "# Create prompt template for LLM\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"\"\"You are a battery engineering expert. Use the provided context to answer the user's question.\n",
    "\n",
    "Context (Retrieved Battery Data):\n",
    "{context}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Instructions:\n",
    "- Answer based only on the provided context\n",
    "- Be technical and precise\n",
    "- If calculating configurations, show step-by-step work\n",
    "- For 2S3P: 2 in series (voltage adds), 3 in parallel (capacity adds)\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Prompt template created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455052cc",
   "metadata": {},
   "source": [
    "## ðŸ¤– Step 9: LLM Setup (API Ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d7d1acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Gemini (flash) LLM initialized successfully!\n",
      "Test response: Yes, I am working.  I'm ready to assist you with y...\n",
      "LLM ready for RAG pipeline\n"
     ]
    }
   ],
   "source": [
    "# LLM Setup - Google Gemini Integration\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get your API key from environment variable\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        google_api_key=google_api_key\n",
    "    )\n",
    "    test_response = llm.invoke(\"Hello, are you working?\")\n",
    "    print(\"Google Gemini (flash) LLM initialized successfully!\")\n",
    "    print(f\"Test response: {test_response.content[:50]}...\")\n",
    "except:\n",
    "    print(\"API key is not working. Please check your API key and try again.\")\n",
    "    llm = None\n",
    "\n",
    "print(\"LLM ready for RAG pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73ee93",
   "metadata": {},
   "source": [
    "## â›“ï¸ Step 10: RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bc00c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG pipeline function ready\n"
     ]
    }
   ],
   "source": [
    "# Complete RAG pipeline function\n",
    "def rag_query(user_query):\n",
    "    \"\"\"Complete RAG pipeline: Query â†’ Search â†’ Context â†’ Prompt â†’ LLM â†’ Response\"\"\"\n",
    "    \n",
    "    print(f\"\\n RAG Pipeline Processing: '{user_query}'\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Semantic Search\n",
    "    print(\"Step 1: Semantic Search\")\n",
    "    retrieved_docs = semantic_search(user_query, top_k=3)\n",
    "    \n",
    "    # Step 2: Format Context\n",
    "    print(\"Step 2: Formatting Context\")\n",
    "    context = \"\\n---\\n\".join(retrieved_docs)\n",
    "    \n",
    "    # Step 3: Create Prompt\n",
    "    print(\"Step 3: Creating Prompt\")\n",
    "    formatted_prompt = prompt_template.format(query=user_query, context=context)\n",
    "    \n",
    "    # Step 4: LLM Processing\n",
    "    print(\"Step 4: LLM Processing\")\n",
    "    try:\n",
    "        llm_response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        # Handle different response types\n",
    "        if hasattr(llm_response, 'content'):\n",
    "            response = llm_response.content  # Gemini response\n",
    "        else:\n",
    "            response = str(llm_response)  # Fallback\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"LLM Error: {e}\")\n",
    "        response = \"Error occurred while processing with LLM. Please check your API key and model availability.\"\n",
    "    \n",
    "    print(\"RAG Pipeline Complete\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": user_query,\n",
    "        \"retrieved_docs\": retrieved_docs,\n",
    "        \"response\": response,\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "print(\"RAG pipeline function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c16e55f",
   "metadata": {},
   "source": [
    "## ðŸ§ª Step 11: Test the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a668fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RAG Pipeline Processing: 'Calculate 2S3P battery configuration'\n",
      "==================================================\n",
      "Step 1: Semantic Search\n",
      "Searching for: 'Calculate 2S3P battery configuration'\n",
      "Retrieved 3 relevant documents\n",
      "Step 2: Formatting Context\n",
      "Step 3: Creating Prompt\n",
      "Step 4: LLM Processing\n",
      "RAG Pipeline Complete\n",
      "\n",
      "RESULT:\n",
      "Query: Calculate 2S3P battery configuration\n",
      "Response: To calculate a 2S3P battery configuration using the provided data, we'll need to select three batteries to be used in parallel and then combine those parallel sets into a series configuration.  Since the question doesn't specify which batteries to use, I will demonstrate using BAT-02722, BAT-04403, and BAT-04262, noting that other combinations are possible.  The best choice would depend on factors not included in the provided data, such as battery consistency and age.\n",
      "\n",
      "\n",
      "**Step 1: Parallel Configuration (3P)**\n",
      "\n",
      "We'll arbitrarily select BAT-02722, BAT-04403, and BAT-04262 for our parallel configuration.  In a parallel configuration, the voltage remains the same, and the capacities add.\n",
      "\n",
      "* **Voltage (V):** Remains the same as the individual battery voltages.  This will be the average voltage due to slight variations in the individual cell voltages.  This is an approximation, as individual cell voltages will vary slightly under load.  The approximate voltage will be (3.62V + 3.67V + 3.67V)/3 = 3.65V\n",
      "\n",
      "* **Capacity (Ah):**  3.02 Ah + 3.0 Ah + 3.3 Ah = 9.32 Ah\n",
      "\n",
      "**Step 2: Series Configuration (2S)**\n",
      "\n",
      "Now, we'll take two of the 3P configurations from Step 1 and connect them in series. In a series configuration, the voltages add, and the capacity remains the same.  This means we need six batteries in total to create a 2S3P configuration, using two identical 3P sets.\n",
      "\n",
      "\n",
      "* **Voltage (V):** 3.65 V + 3.65 V = 7.3 V (approximate, due to voltage variations)\n",
      "\n",
      "* **Capacity (Ah):** Remains the same as the 3P configuration: 9.32 Ah\n",
      "\n",
      "\n",
      "**Result:**\n",
      "\n",
      "A 2S3P battery configuration using BAT-02722, BAT-04403, and BAT-04262 (two sets of these batteries in parallel and then in series) would yield an approximate voltage of 7.3 V and a capacity of 9.32 Ah.  It's crucial to remember that this is an approximation due to the slight variations in individual battery voltages and capacities.  In a real-world scenario, battery management systems (BMS) are necessary to monitor and balance the cells to ensure optimal performance and safety.\n",
      "\n",
      "Retrieved 3 documents for context\n",
      "RAG Pipeline Complete\n",
      "\n",
      "RESULT:\n",
      "Query: Calculate 2S3P battery configuration\n",
      "Response: To calculate a 2S3P battery configuration using the provided data, we'll need to select three batteries to be used in parallel and then combine those parallel sets into a series configuration.  Since the question doesn't specify which batteries to use, I will demonstrate using BAT-02722, BAT-04403, and BAT-04262, noting that other combinations are possible.  The best choice would depend on factors not included in the provided data, such as battery consistency and age.\n",
      "\n",
      "\n",
      "**Step 1: Parallel Configuration (3P)**\n",
      "\n",
      "We'll arbitrarily select BAT-02722, BAT-04403, and BAT-04262 for our parallel configuration.  In a parallel configuration, the voltage remains the same, and the capacities add.\n",
      "\n",
      "* **Voltage (V):** Remains the same as the individual battery voltages.  This will be the average voltage due to slight variations in the individual cell voltages.  This is an approximation, as individual cell voltages will vary slightly under load.  The approximate voltage will be (3.62V + 3.67V + 3.67V)/3 = 3.65V\n",
      "\n",
      "* **Capacity (Ah):**  3.02 Ah + 3.0 Ah + 3.3 Ah = 9.32 Ah\n",
      "\n",
      "**Step 2: Series Configuration (2S)**\n",
      "\n",
      "Now, we'll take two of the 3P configurations from Step 1 and connect them in series. In a series configuration, the voltages add, and the capacity remains the same.  This means we need six batteries in total to create a 2S3P configuration, using two identical 3P sets.\n",
      "\n",
      "\n",
      "* **Voltage (V):** 3.65 V + 3.65 V = 7.3 V (approximate, due to voltage variations)\n",
      "\n",
      "* **Capacity (Ah):** Remains the same as the 3P configuration: 9.32 Ah\n",
      "\n",
      "\n",
      "**Result:**\n",
      "\n",
      "A 2S3P battery configuration using BAT-02722, BAT-04403, and BAT-04262 (two sets of these batteries in parallel and then in series) would yield an approximate voltage of 7.3 V and a capacity of 9.32 Ah.  It's crucial to remember that this is an approximation due to the slight variations in individual battery voltages and capacities.  In a real-world scenario, battery management systems (BMS) are necessary to monitor and balance the cells to ensure optimal performance and safety.\n",
      "\n",
      "Retrieved 3 documents for context\n"
     ]
    }
   ],
   "source": [
    "# Test the complete RAG system\n",
    "test_query = \"Calculate 2S3P battery configuration\"\n",
    "\n",
    "result = rag_query(test_query)\n",
    "\n",
    "print(\"RESULT:\")\n",
    "print(f\"Query: {result['query']}\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"\\nRetrieved {len(result['retrieved_docs'])} documents for context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b5860",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Direct Query Cell - Write Your Question Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6332e0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query: 'which battery has the lowest cost per kwh'\n",
      "============================================================\n",
      "\n",
      " RAG Pipeline Processing: 'which battery has the lowest cost per kwh'\n",
      "==================================================\n",
      "Step 1: Semantic Search\n",
      "Searching for: 'which battery has the lowest cost per kwh'\n",
      "Retrieved 3 relevant documents\n",
      "Step 2: Formatting Context\n",
      "Step 3: Creating Prompt\n",
      "Step 4: LLM Processing\n",
      "LLM Error: Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n",
      "RAG Pipeline Complete\n",
      "\n",
      "\n",
      "============================================================\n",
      "COMPLETE RESULT:\n",
      "============================================================\n",
      "Your Query: which battery has the lowest cost per kwh\n",
      "\n",
      "AI Response:\n",
      "Error occurred while processing with LLM. Please check your API key and model availability.\n",
      "\n",
      "Retrieved 3 relevant documents\n",
      "\n",
      "Context Used:\n",
      "----------------------------------------\n",
      "Document 1:\n",
      "Battery ID: BAT-05042\n",
      "Type: Prismatic\n",
      "Voltage: 3.26 V\n",
      "Capacity: 24.99 Ah\n",
      "Energy Density: 8.9 Wh/kg...\n",
      "\n",
      "Document 2:\n",
      "Battery ID: BAT-05039\n",
      "Type: Prismatic\n",
      "Voltage: 3.28 V\n",
      "Capacity: 29.16 Ah\n",
      "Energy Density: 6.1 Wh/kg...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Change the query below to ask any battery-related question\n",
    "my_query = \"which battery has the lowest cost per kwh\"\n",
    "\n",
    "# Execute RAG pipeline\n",
    "print(f\"Processing your query: '{my_query}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = rag_query(my_query)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLETE RESULT:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Your Query: {result['query']}\")\n",
    "print(f\"\\nAI Response:\\n{result['response']}\")\n",
    "print(f\"\\nRetrieved {len(result['retrieved_docs'])} relevant documents\")\n",
    "print(\"\\nContext Used:\")\n",
    "print(\"-\" * 40)\n",
    "for i, doc in enumerate(result['retrieved_docs'][:2], 1):\n",
    "    print(f\"Document {i}:\\n{doc[:200]}...\\n\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf97228",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 12: Interactive Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7362fb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive chat function ready\n",
      "Run: battery_chat() to start interactive mode\n"
     ]
    }
   ],
   "source": [
    "# Interactive chat function\n",
    "def battery_chat():\n",
    "    \"\"\"Interactive chat using the RAG system\"\"\"\n",
    "    print(\"Battery RAG Chat - Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"Your question: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        if not user_input:\n",
    "            continue\n",
    "            \n",
    "        # Process through RAG pipeline\n",
    "        result = rag_query(user_input)\n",
    "        print(f\"\\nResponse: {result['response']}\\n\")\n",
    "\n",
    "print(\"Interactive chat function ready\")\n",
    "print(\"Run: battery_chat() to start interactive mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af615bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current API Key: AIzaSyC-s-...sDkg\n",
      "Length: 39\n",
      "Direct from .env file: GOOGLE_API_KEY=AIzaSyC-s-RjxvQhK6wVfbmHc1btrPykY_MsDkg\n"
     ]
    }
   ],
   "source": [
    "# Quick API Key Test - Fresh Load\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Force reload of environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Check what's actually loaded\n",
    "current_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(f\"Current API Key: {current_key[:10]}...{current_key[-4:] if current_key else 'None'}\")\n",
    "print(f\"Length: {len(current_key) if current_key else 0}\")\n",
    "\n",
    "# Read directly from .env file to compare\n",
    "with open('.env', 'r') as f:\n",
    "    env_content = f.read()\n",
    "    print(f\"Direct from .env file: {env_content.strip()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
